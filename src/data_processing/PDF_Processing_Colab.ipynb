{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80c2c55f",
      "metadata": {
        "id": "80c2c55f"
      },
      "source": [
        "# Optimized PDF Processing with Docling for Google Colab\n",
        "\n",
        "This notebook processes PDFs using Docling with GPU acceleration, proper chunking (1024 tokens), and batch processing capabilities.\n",
        "\n",
        "## Key Features:\n",
        "- GPU acceleration with CUDA support\n",
        "- Optimized batch processing for speed (8 PDFs per batch for Google Colab)\n",
        "- 1024 token chunks (HybridChunker handles overlap internally)\n",
        "- Proper logging and performance monitoring\n",
        "- Automatic compression of outputs\n",
        "- Compatible with Google Colab GPU runtime\n",
        "\n",
        "## Setup Instructions for Google Colab:\n",
        "1. Set Runtime to GPU: Runtime → Change runtime type → GPU\n",
        "2. Upload your PDFs to a folder named `pdfs` in the root directory\n",
        "3. Run all cells sequentially\n",
        "4. Download the compressed results from the final cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5fc8d0ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc8d0ae",
        "outputId": "26926d2c-c2a7-4e52-9186-e3f9307d265c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "Successfully installed docling\n",
            "Successfully installed docling-core\n",
            "Successfully installed tiktoken\n",
            "Successfully installed openai\n",
            "Successfully installed torch\n",
            "Successfully installed torchvision\n",
            "Successfully installed torchaudio\n",
            "Successfully installed pydantic\n",
            "Successfully installed pydantic-settings\n",
            "All packages installed successfully!\n",
            "GPU detected: Tesla T4\n",
            "CUDA version: 12.4\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup and Dependencies Installation\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package with proper error handling\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to install {package}: {e}\")\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"docling\",\n",
        "    \"docling-core\",\n",
        "    \"tiktoken\",\n",
        "    \"openai\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"pydantic\",\n",
        "    \"pydantic-settings\"\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"All packages installed successfully!\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"No GPU detected - will use CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3ee6ac9d",
      "metadata": {
        "id": "3ee6ac9d"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries and Configure Logging\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import zipfile\n",
        "from collections.abc import Iterable\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Docling imports\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import ConversionStatus, InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
        "from docling.chunking import HybridChunker\n",
        "from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer\n",
        "from docling_core.types.doc import ImageRefMode\n",
        "import tiktoken\n",
        "\n",
        "# Configure comprehensive logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('pdf_processing.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Performance tracking class\n",
        "class PerformanceTracker:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "        self.stage_times = {}\n",
        "\n",
        "    def start_stage(self, stage_name):\n",
        "        self.stage_times[stage_name] = {'start': time.time()}\n",
        "\n",
        "    def end_stage(self, stage_name):\n",
        "        if stage_name in self.stage_times:\n",
        "            self.stage_times[stage_name]['end'] = time.time()\n",
        "            self.stage_times[stage_name]['duration'] = (\n",
        "                self.stage_times[stage_name]['end'] -\n",
        "                self.stage_times[stage_name]['start']\n",
        "            )\n",
        "\n",
        "    def get_stage_duration(self, stage_name):\n",
        "        return self.stage_times.get(stage_name, {}).get('duration', 0)\n",
        "\n",
        "    def get_report(self):\n",
        "        total_time = sum(stage.get('duration', 0) for stage in self.stage_times.values())\n",
        "        report = f\"Performance Report:\\n\"\n",
        "        report += f\"Total Processing Time: {total_time:.2f} seconds\\n\\n\"\n",
        "\n",
        "        for stage, times in self.stage_times.items():\n",
        "            duration = times.get('duration', 0)\n",
        "            percentage = (duration / total_time * 100) if total_time > 0 else 0\n",
        "            report += f\"{stage}: {duration:.2f}s ({percentage:.1f}%)\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "tracker = PerformanceTracker()\n",
        "logger.info(\"Libraries imported and logging configured successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "110ff08b",
      "metadata": {
        "id": "110ff08b"
      },
      "outputs": [],
      "source": [
        "# Configure GPU Acceleration and Pipeline Options\n",
        "def setup_accelerator_options():\n",
        "    \"\"\"Configure optimal accelerator settings based on available hardware\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        accelerator_options = AcceleratorOptions(\n",
        "            num_threads=8,\n",
        "            device=AcceleratorDevice.CUDA\n",
        "        )\n",
        "        logger.info(\"Using CUDA acceleration\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        accelerator_options = AcceleratorOptions(\n",
        "            num_threads=8,\n",
        "            device=AcceleratorDevice.MPS\n",
        "        )\n",
        "        logger.info(\"Using MPS acceleration (Apple Silicon)\")\n",
        "    else:\n",
        "        accelerator_options = AcceleratorOptions(\n",
        "            num_threads=8,\n",
        "            device=AcceleratorDevice.CPU\n",
        "        )\n",
        "        logger.info(\"Using CPU acceleration\")\n",
        "\n",
        "    return accelerator_options\n",
        "\n",
        "def create_optimized_pipeline_options():\n",
        "    \"\"\"Create optimized pipeline options for maximum performance\"\"\"\n",
        "    accelerator_options = setup_accelerator_options()\n",
        "\n",
        "    pipeline_options = PdfPipelineOptions()\n",
        "    pipeline_options.accelerator_options = accelerator_options\n",
        "    pipeline_options.do_ocr = True\n",
        "    pipeline_options.do_table_structure = True\n",
        "    pipeline_options.table_structure_options.do_cell_matching = True\n",
        "    pipeline_options.generate_page_images = False  # Disabled for speed\n",
        "\n",
        "    logger.info(\"Pipeline options configured for optimal performance\")\n",
        "    return pipeline_options\n",
        "\n",
        "# Configuration constants\n",
        "CHUNK_SIZE = 1024\n",
        "BATCH_SIZE = 8  # Optimized for Google Colab GPU - higher throughput\n",
        "\n",
        "logger.info(f\"Processing configuration: chunks={CHUNK_SIZE}, batch_size={BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "53aabca5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53aabca5",
        "outputId": "9fc021f0-2463-4d9c-a3ff-5021ad7dc561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Found 15 PDF files to process.\n",
            "Output will be saved to: /content/processed_documents\n"
          ]
        }
      ],
      "source": [
        "# Setup Directory Structure and File Management\n",
        "def setup_directories():\n",
        "    \"\"\"Create necessary directories and scan for PDF files\"\"\"\n",
        "    # Check if we're in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        is_colab = True\n",
        "        base_path = Path('/content')\n",
        "    except ImportError:\n",
        "        is_colab = False\n",
        "        base_path = Path('.')\n",
        "\n",
        "    # Setup directories\n",
        "    pdfs_dir = base_path / 'pdfs'\n",
        "    output_dir = base_path / 'processed_documents'\n",
        "\n",
        "    if not pdfs_dir.exists():\n",
        "        logger.error(f\"PDFs directory not found: {pdfs_dir}\")\n",
        "        if is_colab:\n",
        "            print(\"Please upload your PDFs to a folder named 'pdfs' in the root directory\")\n",
        "            print(\"You can create the folder and upload files using the file browser on the left\")\n",
        "        return None, None, []\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Scan for PDF files\n",
        "    pdf_files = list(pdfs_dir.glob(\"*.pdf\"))\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.error(\"No PDF files found in the pdfs directory\")\n",
        "        return pdfs_dir, output_dir, []\n",
        "\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files in {pdfs_dir}\")\n",
        "\n",
        "    # Log file details\n",
        "    for pdf_file in pdf_files:\n",
        "        size_mb = pdf_file.stat().st_size / (1024 * 1024)\n",
        "        logger.info(f\"  - {pdf_file.name}: {size_mb:.1f} MB\")\n",
        "\n",
        "    return pdfs_dir, output_dir, pdf_files\n",
        "\n",
        "# Setup directories and scan files\n",
        "pdfs_dir, output_dir, pdf_files = setup_directories()\n",
        "\n",
        "if pdf_files:\n",
        "    print(f\"Setup complete. Found {len(pdf_files)} PDF files to process.\")\n",
        "    print(f\"Output will be saved to: {output_dir}\")\n",
        "else:\n",
        "    print(\"No PDF files found. Please upload PDFs to the 'pdfs' folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b62f9ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b62f9ca",
        "outputId": "7a014567-aef3-44a1-fc8e-9eb953365a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory: 15.8 GB\n",
            "Document processor initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize Document Converter with Optimized Settings\n",
        "class OptimizedDoclingProcessor:\n",
        "    \"\"\"Optimized Docling processor for batch processing with performance monitoring\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: Path):\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Setup pipeline options\n",
        "        pipeline_options = create_optimized_pipeline_options()\n",
        "\n",
        "        # Initialize converter\n",
        "        self.converter = DocumentConverter(\n",
        "            format_options={\n",
        "                InputFormat.PDF: PdfFormatOption(\n",
        "                    pipeline_options=pipeline_options\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Setup tokenizer and chunker for optimal chunk sizes\n",
        "        self.tokenizer = OpenAITokenizer(\n",
        "            tokenizer=tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
        "            max_tokens=CHUNK_SIZE,\n",
        "        )\n",
        "\n",
        "        self.chunker = HybridChunker(\n",
        "            tokenizer=self.tokenizer,\n",
        "            merge_peers=True,\n",
        "        )\n",
        "        # NOTE: HybridChunker handles chunking internally without explicit overlap\n",
        "\n",
        "        logger.info(f\"OptimizedDoclingProcessor initialized with chunk_size={CHUNK_SIZE}\")\n",
        "\n",
        "    def process_batch(self, pdf_paths: List[Path]) -> Dict[str, Any]:\n",
        "        \"\"\"Process a batch of PDFs with detailed performance tracking\"\"\"\n",
        "        batch_start = time.time()\n",
        "\n",
        "        logger.info(f\"Starting batch processing of {len(pdf_paths)} PDFs\")\n",
        "\n",
        "        # Track conversion stage\n",
        "        tracker.start_stage(\"Document_Conversion\")\n",
        "\n",
        "        conv_results = self.converter.convert_all(\n",
        "            pdf_paths,\n",
        "            raises_on_error=False\n",
        "        )\n",
        "\n",
        "        tracker.end_stage(\"Document_Conversion\")\n",
        "        conversion_time = tracker.get_stage_duration(\"Document_Conversion\")\n",
        "\n",
        "        # Process results\n",
        "        tracker.start_stage(\"Chunking_and_Export\")\n",
        "\n",
        "        results = {\n",
        "            'success_count': 0,\n",
        "            'partial_count': 0,\n",
        "            'failure_count': 0,\n",
        "            'total_chunks': 0,\n",
        "            'processed_files': [],\n",
        "            'failed_files': [],\n",
        "            'conversion_time': conversion_time,\n",
        "            'processing_details': []\n",
        "        }\n",
        "\n",
        "        for conv_res in conv_results:\n",
        "            pdf_name = conv_res.input.file.stem\n",
        "            file_start = time.time()\n",
        "\n",
        "            try:\n",
        "                if conv_res.status == ConversionStatus.SUCCESS:\n",
        "                    # Chunk the document\n",
        "                    chunks = list(self.chunker.chunk(conv_res.document))\n",
        "\n",
        "                    # Export with optimized settings\n",
        "                    self._export_document(conv_res, chunks)\n",
        "\n",
        "                    file_time = time.time() - file_start\n",
        "                    results['success_count'] += 1\n",
        "                    results['total_chunks'] += len(chunks)\n",
        "                    results['processed_files'].append(pdf_name)\n",
        "\n",
        "                    details = {\n",
        "                        'file': pdf_name,\n",
        "                        'status': 'success',\n",
        "                        'chunks': len(chunks),\n",
        "                        'processing_time': file_time\n",
        "                    }\n",
        "                    results['processing_details'].append(details)\n",
        "\n",
        "                    logger.info(f\"SUCCESS: {pdf_name} - {len(chunks)} chunks ({file_time:.1f}s)\")\n",
        "\n",
        "                elif conv_res.status == ConversionStatus.PARTIAL_SUCCESS:\n",
        "                    results['partial_count'] += 1\n",
        "                    logger.warning(f\"PARTIAL SUCCESS: {pdf_name}\")\n",
        "                else:\n",
        "                    results['failure_count'] += 1\n",
        "                    results['failed_files'].append(pdf_name)\n",
        "                    logger.error(f\"CONVERSION FAILED: {pdf_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                results['failure_count'] += 1\n",
        "                results['failed_files'].append(pdf_name)\n",
        "                logger.error(f\"PROCESSING ERROR: {pdf_name} - {e}\")\n",
        "\n",
        "        tracker.end_stage(\"Chunking_and_Export\")\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        results['batch_time'] = batch_time\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _extract_docling_metadata(self, chunk) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract rich metadata from Docling chunk - PROPER JSON serializable format.\n",
        "        Based on the original docling_processor.py implementation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            meta = {}\n",
        "\n",
        "            # Extract dl_meta structure properly\n",
        "            if hasattr(chunk, 'meta') and chunk.meta:\n",
        "                if hasattr(chunk.meta, 'doc_items') and chunk.meta.doc_items:\n",
        "                    doc_items = []\n",
        "                    for item in chunk.meta.doc_items:\n",
        "                        doc_item = {\n",
        "                            'self_ref': getattr(item, 'self_ref', ''),\n",
        "                            'label': getattr(item, 'label', 'unknown'),\n",
        "                        }\n",
        "\n",
        "                        # Extract bbox if available\n",
        "                        if hasattr(item, 'prov') and item.prov:\n",
        "                            prov = item.prov[0] if item.prov else None\n",
        "                            if prov:\n",
        "                                doc_item['page_no'] = getattr(prov, 'page_no', 0)\n",
        "                                # Convert BoundingBox to serializable dict\n",
        "                                bbox = getattr(prov, 'bbox', None)\n",
        "                                if bbox:\n",
        "                                    doc_item['bbox'] = {\n",
        "                                        'l': getattr(bbox, 'l', 0),\n",
        "                                        't': getattr(bbox, 't', 0),\n",
        "                                        'r': getattr(bbox, 'r', 0),\n",
        "                                        'b': getattr(bbox, 'b', 0)\n",
        "                                    }\n",
        "                                else:\n",
        "                                    doc_item['bbox'] = {}\n",
        "\n",
        "                        doc_items.append(doc_item)\n",
        "\n",
        "                    meta['doc_items'] = doc_items\n",
        "\n",
        "                # Extract headings\n",
        "                if hasattr(chunk.meta, 'headings') and chunk.meta.headings:\n",
        "                    meta['headings'] = list(chunk.meta.headings)\n",
        "\n",
        "                # Extract origin\n",
        "                if hasattr(chunk.meta, 'origin'):\n",
        "                    origin = chunk.meta.origin\n",
        "                    meta['origin'] = {\n",
        "                        'mimetype': getattr(origin, 'mimetype', ''),\n",
        "                        'filename': getattr(origin, 'filename', ''),\n",
        "                        'binary_hash': getattr(origin, 'binary_hash', None)\n",
        "                    }\n",
        "\n",
        "            return meta\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not extract Docling metadata: {e}\")\n",
        "            return {'doc_items': [], 'headings': []}\n",
        "\n",
        "    def _export_document(self, conv_res, chunks):\n",
        "        \"\"\"Export document in optimized formats\"\"\"\n",
        "        doc_filename = conv_res.input.file.stem\n",
        "\n",
        "        # Prepare chunk data with proper metadata\n",
        "        chunk_data = {\n",
        "            'metadata': {\n",
        "                'source_file': conv_res.input.file.name,\n",
        "                'processing_timestamp': datetime.now().isoformat(),\n",
        "                'chunk_count': len(chunks),\n",
        "                'chunking_method': 'docling_hybrid_optimized',\n",
        "                'tokenizer': 'openai_tiktoken',\n",
        "                'chunk_size': CHUNK_SIZE,\n",
        "                'docling_version': 'latest'\n",
        "            },\n",
        "            'chunks': []\n",
        "        }\n",
        "\n",
        "        # Process chunks with enhanced metadata\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            # PROPER context enrichment using chunker.contextualize()\n",
        "            enriched_content = self.chunker.contextualize(chunk=chunk)\n",
        "\n",
        "            # Extract metadata properly to avoid JSON serialization errors\n",
        "            docling_meta = self._extract_docling_metadata(chunk)\n",
        "\n",
        "            chunk_info = {\n",
        "                'id': f\"{doc_filename}_{idx}\",\n",
        "                'content': chunk.text,\n",
        "                'context_enriched_content': enriched_content,\n",
        "                'chunk_index': idx,\n",
        "                'token_count': len(self.tokenizer.tokenizer.encode(chunk.text)),\n",
        "                'docling_meta': docling_meta\n",
        "            }\n",
        "            chunk_data['chunks'].append(chunk_info)\n",
        "\n",
        "        # Save JSON file\n",
        "        json_path = self.output_dir / f\"{doc_filename}_docling_chunks.json\"\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunk_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save readable TXT file\n",
        "        txt_path = self.output_dir / f\"{doc_filename}_docling_chunks.txt\"\n",
        "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Source: {conv_res.input.file.name}\\n\")\n",
        "            f.write(f\"Processed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Total Chunks: {len(chunks)}\\n\")\n",
        "            f.write(f\"Chunk Size: {CHUNK_SIZE} tokens\\n\")\n",
        "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                f.write(f\"CHUNK {idx + 1}:\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\")\n",
        "                f.write(chunk.text)\n",
        "                f.write(\"\\n\\n\" + \"=\" * 40 + \"\\n\\n\")\n",
        "\n",
        "# Initialize processor if we have files to process\n",
        "if pdf_files and output_dir:\n",
        "    processor = OptimizedDoclingProcessor(output_dir)\n",
        "    print(\"Document processor initialized successfully\")\n",
        "else:\n",
        "    print(\"Cannot initialize processor - no PDF files found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "35701601",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35701601",
        "outputId": "8f63e675-cb54-4ff7-a208-68c8228b764e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting optimized PDF processing...\n",
            "Processing 15 PDFs in 2 batches of 8\n",
            "============================================================\n",
            "Batch 1/2: Processing 8 files\n",
            "Batch 1 completed:\n",
            "  Success: 8\n",
            "  Partial: 0\n",
            "  Failed: 0\n",
            "  Chunks: 1804\n",
            "  Time: 1376.7s\n",
            "----------------------------------------\n",
            "Batch 2/2: Processing 7 files\n",
            "Batch 2 completed:\n",
            "  Success: 7\n",
            "  Partial: 0\n",
            "  Failed: 0\n",
            "  Chunks: 1789\n",
            "  Time: 1618.9s\n",
            "----------------------------------------\n",
            "\n",
            "PROCESSING COMPLETE!\n",
            "============================================================\n",
            "Total Files: 15\n",
            "Successful: 15\n",
            "Partial Success: 0\n",
            "Failed: 0\n",
            "Total Chunks Generated: 3593\n",
            "Total Time: 2996.0 seconds\n",
            "Average per file: 199.7 seconds\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Batch PDF Processing with Performance Monitoring\n",
        "def process_all_pdfs_optimized():\n",
        "    \"\"\"Process all PDFs with comprehensive monitoring and reporting\"\"\"\n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files to process\")\n",
        "        return None\n",
        "\n",
        "    tracker.start_stage(\"Total_Processing\")\n",
        "\n",
        "    total_results = {\n",
        "        'total_files': len(pdf_files),\n",
        "        'total_success': 0,\n",
        "        'total_partial': 0,\n",
        "        'total_failure': 0,\n",
        "        'total_chunks': 0,\n",
        "        'batch_results': [],\n",
        "        'processing_start': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Process in batches\n",
        "    total_batches = (len(pdf_files) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    print(f\"Processing {len(pdf_files)} PDFs in {total_batches} batches of {BATCH_SIZE}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i in range(0, len(pdf_files), BATCH_SIZE):\n",
        "        batch_num = i // BATCH_SIZE + 1\n",
        "        batch = pdf_files[i:i + BATCH_SIZE]\n",
        "\n",
        "        print(f\"Batch {batch_num}/{total_batches}: Processing {len(batch)} files\")\n",
        "\n",
        "        batch_results = processor.process_batch(batch)\n",
        "        total_results['batch_results'].append(batch_results)\n",
        "\n",
        "        # Update totals\n",
        "        total_results['total_success'] += batch_results['success_count']\n",
        "        total_results['total_partial'] += batch_results['partial_count']\n",
        "        total_results['total_failure'] += batch_results['failure_count']\n",
        "        total_results['total_chunks'] += batch_results['total_chunks']\n",
        "\n",
        "        # Progress update\n",
        "        print(f\"Batch {batch_num} completed:\")\n",
        "        print(f\"  Success: {batch_results['success_count']}\")\n",
        "        print(f\"  Partial: {batch_results['partial_count']}\")\n",
        "        print(f\"  Failed: {batch_results['failure_count']}\")\n",
        "        print(f\"  Chunks: {batch_results['total_chunks']}\")\n",
        "        print(f\"  Time: {batch_results['batch_time']:.1f}s\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    tracker.end_stage(\"Total_Processing\")\n",
        "\n",
        "    total_results['processing_end'] = datetime.now().isoformat()\n",
        "    total_results['total_time'] = tracker.get_stage_duration(\"Total_Processing\")\n",
        "\n",
        "    return total_results\n",
        "\n",
        "# Run the processing\n",
        "if pdf_files:\n",
        "    print(\"Starting optimized PDF processing...\")\n",
        "    results = process_all_pdfs_optimized()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nPROCESSING COMPLETE!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total Files: {results['total_files']}\")\n",
        "        print(f\"Successful: {results['total_success']}\")\n",
        "        print(f\"Partial Success: {results['total_partial']}\")\n",
        "        print(f\"Failed: {results['total_failure']}\")\n",
        "        print(f\"Total Chunks Generated: {results['total_chunks']}\")\n",
        "        print(f\"Total Time: {results['total_time']:.1f} seconds\")\n",
        "        print(f\"Average per file: {results['total_time']/results['total_files']:.1f} seconds\")\n",
        "        print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"No files to process\")\n",
        "    results = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "29577752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "29577752",
        "outputId": "30431f48-f4b5-4675-ad66-eef2997c7651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating performance report...\n",
            "\n",
            "DETAILED PERFORMANCE REPORT\n",
            "==================================================\n",
            "Processing Date: 2025-08-18T19:58:15.323078\n",
            "Total Processing Time: 2995.96 seconds\n",
            "Average Time per PDF: 199.73 seconds\n",
            "\n",
            "SUMMARY:\n",
            "  Total Files: 15\n",
            "  Successful: 15\n",
            "  Partial Success: 0\n",
            "  Failed: 0\n",
            "  Success Rate: 100.0%\n",
            "  Total Chunks: 3593\n",
            "  Average Chunks per PDF: 239.5\n",
            "\n",
            "PERFORMANCE BREAKDOWN:\n",
            "Performance Report:\n",
            "Total Processing Time: 4614.88 seconds\n",
            "\n",
            "Total_Processing: 2995.96s (64.9%)\n",
            "Document_Conversion: 0.00s (0.0%)\n",
            "Chunking_and_Export: 1618.93s (35.1%)\n",
            "\n",
            "\n",
            "BATCH DETAILS:\n",
            "  Batch 1:\n",
            "    Conversion Time: 0.00s\n",
            "    Total Batch Time: 1376.75s\n",
            "    Success: 8\n",
            "    Failed: 0\n",
            "\n",
            "  Batch 2:\n",
            "    Conversion Time: 0.00s\n",
            "    Total Batch Time: 1618.93s\n",
            "    Success: 7\n",
            "    Failed: 0\n",
            "\n",
            "BOTTLENECK ANALYSIS:\n",
            "  Document Conversion: 0.00s (0.0%)\n",
            "  Chunking & Export: 2995.96s (100.0%)\n",
            "  RECOMMENDATION: Chunking/Export is the bottleneck. Consider:\n",
            "    - Increasing chunk sizes\n",
            "    - Optimizing JSON serialization\n",
            "    - Using faster storage\n",
            "\n",
            "Compressing outputs...\n",
            "Outputs compressed to: /content/processed_documents_20250818_204811.zip\n",
            "Archive size: 4.6 MB\n",
            "\n",
            "Downloading compressed archive...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_db9b59b5-f61d-49b8-a7f8-435af5585fe2\", \"processed_documents_20250818_204811.zip\", 4799544)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download started! Check your browser's downloads folder.\n",
            "Performance report saved to: /content/processed_documents/performance_report.txt\n",
            "\n",
            "Processing pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Generate Performance Reports and Compression\n",
        "def create_performance_report(results):\n",
        "    \"\"\"Generate detailed performance report\"\"\"\n",
        "    if not results:\n",
        "        return \"No results to report\"\n",
        "\n",
        "    report = []\n",
        "    report.append(\"DETAILED PERFORMANCE REPORT\")\n",
        "    report.append(\"=\" * 50)\n",
        "    report.append(f\"Processing Date: {results['processing_start']}\")\n",
        "    report.append(f\"Total Processing Time: {results['total_time']:.2f} seconds\")\n",
        "    report.append(f\"Average Time per PDF: {results['total_time']/results['total_files']:.2f} seconds\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    report.append(\"SUMMARY:\")\n",
        "    report.append(f\"  Total Files: {results['total_files']}\")\n",
        "    report.append(f\"  Successful: {results['total_success']}\")\n",
        "    report.append(f\"  Partial Success: {results['total_partial']}\")\n",
        "    report.append(f\"  Failed: {results['total_failure']}\")\n",
        "    report.append(f\"  Success Rate: {(results['total_success']/results['total_files']*100):.1f}%\")\n",
        "    report.append(f\"  Total Chunks: {results['total_chunks']}\")\n",
        "    report.append(f\"  Average Chunks per PDF: {results['total_chunks']/max(results['total_success'], 1):.1f}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    report.append(\"PERFORMANCE BREAKDOWN:\")\n",
        "    report.append(tracker.get_report())\n",
        "    report.append(\"\")\n",
        "\n",
        "    report.append(\"BATCH DETAILS:\")\n",
        "    for i, batch in enumerate(results['batch_results'], 1):\n",
        "        report.append(f\"  Batch {i}:\")\n",
        "        report.append(f\"    Conversion Time: {batch['conversion_time']:.2f}s\")\n",
        "        report.append(f\"    Total Batch Time: {batch['batch_time']:.2f}s\")\n",
        "        report.append(f\"    Success: {batch['success_count']}\")\n",
        "        report.append(f\"    Failed: {batch['failure_count']}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Identify potential bottlenecks\n",
        "    conversion_time = sum(b['conversion_time'] for b in results['batch_results'])\n",
        "    processing_time = results['total_time'] - conversion_time\n",
        "\n",
        "    report.append(\"BOTTLENECK ANALYSIS:\")\n",
        "    report.append(f\"  Document Conversion: {conversion_time:.2f}s ({conversion_time/results['total_time']*100:.1f}%)\")\n",
        "    report.append(f\"  Chunking & Export: {processing_time:.2f}s ({processing_time/results['total_time']*100:.1f}%)\")\n",
        "\n",
        "    if conversion_time > processing_time * 2:\n",
        "        report.append(\"  RECOMMENDATION: Document conversion is the bottleneck. Consider:\")\n",
        "        report.append(\"    - Reducing OCR quality settings\")\n",
        "        report.append(\"    - Disabling table structure detection for simple documents\")\n",
        "        report.append(\"    - Using smaller batch sizes\")\n",
        "    elif processing_time > conversion_time * 2:\n",
        "        report.append(\"  RECOMMENDATION: Chunking/Export is the bottleneck. Consider:\")\n",
        "        report.append(\"    - Increasing chunk sizes\")\n",
        "        report.append(\"    - Optimizing JSON serialization\")\n",
        "        report.append(\"    - Using faster storage\")\n",
        "    else:\n",
        "        report.append(\"  ANALYSIS: Processing is well-balanced\")\n",
        "\n",
        "    return \"\\n\".join(report)\n",
        "\n",
        "def compress_outputs():\n",
        "    \"\"\"Compress all output files for easy download\"\"\"\n",
        "    if not output_dir or not output_dir.exists():\n",
        "        return None\n",
        "\n",
        "    # Create ZIP file\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    zip_path = output_dir.parent / f\"processed_documents_{timestamp}.zip\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Add all processed files\n",
        "        for file_path in output_dir.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                zipf.write(file_path, file_path.relative_to(output_dir))\n",
        "\n",
        "        # Add performance report\n",
        "        if results:\n",
        "            report_content = create_performance_report(results)\n",
        "            zipf.writestr(\"performance_report.txt\", report_content)\n",
        "\n",
        "        # Add processing log\n",
        "        if Path('pdf_processing.log').exists():\n",
        "            zipf.write('pdf_processing.log', 'pdf_processing.log')\n",
        "\n",
        "    return zip_path\n",
        "\n",
        "# Generate reports and compress outputs\n",
        "if results:\n",
        "    print(\"Generating performance report...\")\n",
        "    report = create_performance_report(results)\n",
        "    print(\"\\n\" + report)\n",
        "\n",
        "    print(\"\\nCompressing outputs...\")\n",
        "    zip_path = compress_outputs()\n",
        "\n",
        "    if zip_path:\n",
        "        zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"Outputs compressed to: {zip_path}\")\n",
        "        print(f\"Archive size: {zip_size_mb:.1f} MB\")\n",
        "\n",
        "        # For Google Colab users\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"\\nDownloading compressed archive...\")\n",
        "            files.download(str(zip_path))\n",
        "            print(\"Download started! Check your browser's downloads folder.\")\n",
        "        except ImportError:\n",
        "            print(f\"Archive ready for download: {zip_path}\")\n",
        "\n",
        "    # Save performance report separately\n",
        "    if output_dir:\n",
        "        report_path = output_dir / \"performance_report.txt\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report)\n",
        "        print(f\"Performance report saved to: {report_path}\")\n",
        "\n",
        "print(\"\\nProcessing pipeline completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}